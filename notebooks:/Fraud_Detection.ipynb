{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Rkuoi_34hNh"
   },
   "source": [
    "## 📥 Data Ingestion with Dask\n",
    "\n",
    "I started by using Dask to simulate working with large datasets. Although the dataset fits in memory, Dask ensures scalability and future-proofing for real-world big data scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2S611EV-Paj"
   },
   "source": [
    "I began by utilizing the Kaggle API to fetch the Credit Card Fraud Detection dataset. Subsequently, I extracted the dataset from its compressed form and employed Dask to emulate managing extensive datasets. While the data is well-suited for memory storage, this method ensures scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "M5B8UMgb4eu7",
    "outputId": "efdfd978-93d6-4366-8775-d60e990ff169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-7f4eade5-b6a1-4c3b-989d-d6102a9b5eea\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-7f4eade5-b6a1-4c3b-989d-d6102a9b5eea\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle (2).json\n",
      "Dataset URL: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
      "License(s): DbCL-1.0\n",
      "creditcardfraud.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  creditcardfraud.zip\n",
      "  inflating: creditcard.csv          \n",
      "Dataset shape (approx): (284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and Unzip Dataset using Kaggle API\n",
    "\n",
    "# Upload your kaggle.json file from your local machine.\n",
    "from google.colab import files\n",
    "files.upload()  # Select your kaggle.json file when prompted\n",
    "\n",
    "# Configure Kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download the dataset (if not already downloaded)\n",
    "!kaggle datasets download -d mlg-ulb/creditcardfraud\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip -o creditcardfraud.zip\n",
    "\n",
    "# Step 2: Load the Dataset with Dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Use assume_missing=True to handle any dtype issues\n",
    "ddf = dd.read_csv('creditcard.csv', assume_missing=True)\n",
    "\n",
    "# Compute the number of rows (columns are known already)\n",
    "shape = (ddf.shape[0].compute(), ddf.shape[1])\n",
    "print(\"Dataset shape (approx):\", shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQhem8t35IMW"
   },
   "source": [
    "## 🔍 Exploratory Data Analysis & Preprocessing\n",
    "\n",
    "After loading the dataset, I converted it to a Pandas DataFrame for analysis. I dropped the 'Time' column, which wasn’t useful for this task, and applied `StandardScaler` to normalize the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOouYOWF_QPw"
   },
   "source": [
    "Then, I used Pandas DataFrame based on the Dask DataFrame to complete the further analysis and preprocessing. Relevant for this sented, I looked at the dataset, dropped the \"Time\" column, (not useful for this task) and then standardized the features using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9I-OGSSi5N1R",
    "outputId": "5a6e32f4-1587-4d45-cbfb-2cd7f500f27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame shape: (284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
      "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
      "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
      "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "                Time            V1            V2            V3            V4  \\\n",
      "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
      "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
      "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
      "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
      "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
      "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
      "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
      "\n",
      "                 V5            V6            V7            V8            V9  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
      "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
      "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
      "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
      "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
      "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
      "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
      "\n",
      "       ...           V21           V22           V23           V24  \\\n",
      "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
      "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
      "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
      "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
      "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
      "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
      "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
      "\n",
      "                V25           V26           V27           V28         Amount  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
      "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
      "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
      "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
      "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
      "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
      "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
      "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
      "\n",
      "               Class  \n",
      "count  284807.000000  \n",
      "mean        0.001727  \n",
      "std         0.041527  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 31 columns]\n",
      "Fraudulent transactions count:\n",
      " Class\n",
      "0.0    284315\n",
      "1.0       492\n",
      "Name: count, dtype: int64\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Data Exploration and Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert Dask DataFrame to Pandas DataFrame\n",
    "data = ddf.compute()\n",
    "print(\"Pandas DataFrame shape:\", data.shape)\n",
    "print(data.head())\n",
    "\n",
    "# Display basic statistics and check the class distribution\n",
    "print(data.describe())\n",
    "print(\"Fraudulent transactions count:\\n\", data['Class'].value_counts())\n",
    "\n",
    "# Drop 'Time' and separate features (X) and target (y)\n",
    "X = data.drop(['Time', 'Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceiKCkBH5S3K"
   },
   "source": [
    "## **Model Fitting and Evaluation Using LightGBM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOOJMILn_go_"
   },
   "source": [
    "Finally, I split the data into training and test sets, using stratified sampling to preserve the class imbalance. As the data is heavily imbalanced (few rows of fraud), I calculated scale_pos_weight parameter to help LightGBM to prioritize the minority class. We trained the model with early stopping using a callback and then calculated metrics: ROC-AUC, F1 score, and a classification report. I also experimented with tuning the decision threshold to increase the concentration of fraudulent cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6SUnlJv5W7T",
    "outputId": "e4ea398e-04aa-4586-e282-dba1adf61778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Pos Weight: 577.2868020304569\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.90548\n",
      "Default threshold (0.5) results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99     56864\n",
      "         1.0       0.09      0.87      0.16        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.54      0.93      0.58     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n",
      "ROC-AUC Score: 0.9035172893721359\n",
      "F1 Score: 0.16221374045801526\n",
      "Adjusted threshold (0.3) results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99     56864\n",
      "         1.0       0.09      0.87      0.16        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.54      0.93      0.58     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n",
      "F1 Score: 0.1619047619047619\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Training and Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Calculate scale_pos_weight = (# negatives)/(# positives)\n",
    "scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "print(\"Scale Pos Weight:\", scale_pos_weight)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "# Train the model with early stopping via callback\n",
    "num_round = 100\n",
    "bst = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_round,\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = bst.predict(X_test)\n",
    "\n",
    "# Option 1: Using the default threshold of 0.5\n",
    "y_pred_default = (y_pred_prob >= 0.5).astype(int)\n",
    "print(\"Default threshold (0.5) results:\")\n",
    "print(classification_report(y_test, y_pred_default))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_default))\n",
    "\n",
    "# Option 2: Adjust the threshold (e.g., 0.3) for better balance\n",
    "threshold = 0.3\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "print(f\"Adjusted threshold ({threshold}) results:\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O8PFU2C_rWq"
   },
   "source": [
    "In this experiment, adjusting the threshold can improve the recall for the fraud class, although you may need to fine-tune it further based on your desired trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i-iGPa9_s-0"
   },
   "source": [
    "## **Saving the Model and Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcAuAxxG_ycR",
    "outputId": "771f262f-abd9-4a7f-d2b1-460036d2e0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save the Model and Scaler\n",
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(bst, \"lgb_model.pkl\")\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"Model and scaler saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
