# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q_3vWqtIEwrUsFcbMFAkvocJTpUPW797

## Data Ingestion with Dask

I started by using Dask to simulate working with large datasets. Although the dataset fits in memory, Dask ensures scalability and future-proofing for real-world big data scenarios.
"""

# Step 1: Download and Unzip Dataset using Kaggle API

# Upload your kaggle.json file from your local machine.
from google.colab import files
files.upload()  # Select your kaggle.json file when prompted

# Configure Kaggle credentials
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset (if not already downloaded)
!kaggle datasets download -d mlg-ulb/creditcardfraud

# Unzip the dataset
!unzip -o creditcardfraud.zip

# Step 2: Load the Dataset with Dask
import dask.dataframe as dd

# Use assume_missing=True to handle any dtype issues
ddf = dd.read_csv('creditcard.csv', assume_missing=True)

# Compute the number of rows (columns are known already)
shape = (ddf.shape[0].compute(), ddf.shape[1])
print("Dataset shape (approx):", shape)

"""## Exploratory Data Analysis & Preprocessing

After loading the dataset, I converted it to a Pandas DataFrame for analysis. I dropped the 'Time' column, which wasn’t useful for this task, and applied `StandardScaler` to normalize the features.
"""

# Step 3: Data Exploration and Preprocessing
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Convert Dask DataFrame to Pandas DataFrame
data = ddf.compute()
print("Pandas DataFrame shape:", data.shape)
print(data.head())

# Display basic statistics and check the class distribution
print(data.describe())
print("Fraudulent transactions count:\n", data['Class'].value_counts())

# Drop 'Time' and separate features (X) and target (y)
X = data.drop(['Time', 'Class'], axis=1)
y = data['Class']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
print("Preprocessing complete.")

"""## **Model Fitting and Evaluation Using LightGBM**

Finally, I split the data into training and test sets, using stratified sampling to preserve the class imbalance. As the data is heavily imbalanced (few rows of fraud), I calculated scale_pos_weight parameter to help LightGBM to prioritize the minority class. We trained the model with early stopping using a callback and then calculated metrics: ROC-AUC, F1 score, and a classification report. I also experimented with tuning the decision threshold to increase the concentration of fraudulent cases.
"""

# Step 4: Model Training and Evaluation
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.metrics import classification_report, roc_auc_score, f1_score

# Stratified train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Calculate scale_pos_weight = (# negatives)/(# positives)
scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()
print("Scale Pos Weight:", scale_pos_weight)

# Create LightGBM datasets
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# LightGBM parameters
params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting': 'gbdt',
    'learning_rate': 0.05,
    'verbose': -1,
    'scale_pos_weight': scale_pos_weight
}

# Train the model with early stopping via callback
num_round = 100
bst = lgb.train(
    params,
    train_data,
    num_round,
    valid_sets=[test_data],
    callbacks=[lgb.early_stopping(stopping_rounds=10)]
)

# Predict probabilities on the test set
y_pred_prob = bst.predict(X_test)

# Option 1: Using the default threshold of 0.5
y_pred_default = (y_pred_prob >= 0.5).astype(int)
print("Default threshold (0.5) results:")
print(classification_report(y_test, y_pred_default))
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_prob))
print("F1 Score:", f1_score(y_test, y_pred_default))

# Option 2: Adjust the threshold (e.g., 0.3) for better balance
threshold = 0.3
y_pred_adjusted = (y_pred_prob >= threshold).astype(int)
print(f"Adjusted threshold ({threshold}) results:")
print(classification_report(y_test, y_pred_adjusted))
print("F1 Score:", f1_score(y_test, y_pred_adjusted))

"""In this experiment, adjusting the threshold can improve the recall for the fraud class, although you may need to fine-tune it further based on your desired trade-off between precision and recall.

## **Saving the Model and Scaler**
"""

# Step 5: Save the Model and Scaler
import joblib

# Save the model
joblib.dump(bst, "lgb_model.pkl")
# Save the scaler
joblib.dump(scaler, "scaler.pkl")

print("Model and scaler saved successfully.")